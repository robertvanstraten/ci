# Deel 3: Evaluatie
Lever je antwoorden (als .pdf) onderaan deze pagina in. **Eén submit per groep.**

**Deadline: woensdag 20 mei**

### 3.0 Taakverdeling (update)
Zijn er dingen veranderd ten op zichte van vorige week?
\[max 150 woorden\]

### 3.1 Methode
Leg uit hoe je de algoritmes hebt geëvalueerd. Probeer hierbij genoeg informatie te geven om de lezer in staat te stellen om jouw resultaten te reproduceren. Denk hierbij onder andere aan de volgende vragen: Welke maten gebruik je? Wat zeggen deze maten over de resultaten? Heb je de maten toegepast op de hele dataset of op een subset? In het laatste geval, welke subset? Als van toepassing, hoe heb je een testset gegenereerd? Hoe groot is de testset, hoe groot is de traingingset? Welke baseline(s) gebruik je om mee te vergelijken?

\[max 500 woorden\]
### 3.2 Resultaten
Laat de resultaten zien. Als je plots gebruikt, leg deze uit. Zorg ervoor dat de lezer snapt waar ze naar kijkt.

\[max 300 woorden\]

### 3.3 Bespreking
Interpreteer de resultaten. Wat zeggen de resultaten over het algoritmes? Doen ze het beter dan de baseline? Is er een algoritme dat duidelijk beter dan de ander? Als er een verschil is, waardoor komt dat?   
\[max 500 woorden\]

## Submit

Lever hieronder dit verslag *plus de eerdere twee* verslagen. Lever ook je code in, het liefst als GitHub link. Als je geen GitHub hebt gebruikt kan je ook een link naar een zip-file meegeven. In het laaste geval graag een zip *zonder de yelp-data*, anders wordt het bestand te groot. Zet de namen van alle groepsleden op de verslagen. Laat één persoon alles inleveren namens de hele groep.
